{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "1. Please copy the code from the previous assignment (week 2) into a separate file `blocks.py`. Make sure it resides in the same folder as this notebook. It should contain the implementation of the building blocks. \n",
    "2. Downlod the files from [here](http://yann.lecun.com/exdb/mnist/) and place them next to this notebook. You should have 4 files: `t10k-images-idx3-ubyte\n",
    "3. All functions should be implemented using [**NumPy**](https://docs.scipy.org/doc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to supply you with the **building blocks** of **neural networks** (NNs). In this notebook, we will cover the main aspects of NNs, such as **Backpropagation** and **Optimization Methods**. \n",
    "You will understand how **Convolutional Neural Networks** and the basics of **image filtering** work. We will implement matrix convolution as well as the convolutional layer from scratch.\n",
    "\n",
    "\n",
    "### Note\n",
    "Some of the concepts below have not (yet) been discussed during the lecture. These will be discussed further during the next lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "* [1. Fully-Connected Neural Networks](#1.-Fully-Connected-Neural-Networks)\n",
    "    * [1.1 Backpropagation](#1.1-Backpropagation)\n",
    "    * [1.2 Dense layer](#1.2-Dense-layer)\n",
    "    * [1.3 ReLU nonlinearity](#1.3-ReLU-nonlinearity)\n",
    "    * [1.4 Sigmoid nonlinearity](#1.4-Sigmoid-nonlinearity)\n",
    "    * [1.5 Sequential model](#1.5-Sequential-model)\n",
    "    * [1.6 NLL loss function](#1.6-NLL-loss-function)\n",
    "    * [1.7 $L_2$ regularization](#1.7-$L_2$-regularization)\n",
    "    * [1.8 SGD optimizer](#1.8-SGD-optimizer)\n",
    "* [2. Experiments](#2.-Experiments)\n",
    "* [3. Convolutions](#3.-Convolutions)\n",
    "    * [3.1 Matrix convolution](#3.1-Matrix-convolution)\n",
    "    * [3.2 Basic kernels](#3.2-Matrix-convolution)\n",
    "    * [3.3 Convolutional layer](#3.3-Convolutional-layer)\n",
    "    * [3.4 Pooling layer](#3.4-Pooling-layer)\n",
    "    * [3.5 Flatten](#3.5-Flatten)\n",
    "* [4. Image Experiments](#4.-Image-Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import blocks\n",
    "\n",
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = 'YOUR USERNMAE'\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fully-Connected Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Backpropagation\n",
    "\n",
    "Neural networks consist of several layers. Each layer is a function of several parameters that we call weights: $h = f(x, w)$ where $h$ is the layer, $x$ is a vector of inputs and w is a vector of weights. \n",
    "In the neural network, the output of one layer is the input for the next layer. This means we can chain the different functions. The whole neural network $F$ then becomes a composition of different functions. \n",
    "$$\n",
    "F = f_k \\circ f_{k-1} \\circ \\dots \\ f_1\\\\\n",
    "h_1 = f_1(x, w_1)\\\\\n",
    "h_2 = f_2(h_1, w_2)\\\\\n",
    "\\dots \\\\\n",
    "\\dot{y} = f_k(h_{k-1}, w_k)\n",
    "$$\n",
    "In the above functions, $w_1$ and $w_2$ are different **weight vectors** that apply to the different layers $h_1$ and $h_2$. The weights of a neural network basically determine the effect certain outputs have on the next layer. (Please note: When searching for these terms on the internet, be aware that **weights** are sometimes called **parameters**, and $w$ is sometimes denoted as $\\theta$.) \n",
    "\n",
    "\n",
    "At the end of every neural network, there is a loss function. A loss function calculates for the performance of the Neural Network. The calculation of this score depends on the task at hand. For classification tasks the loss function would calculate the difference between prediction and the correct value. In this case the function is a summation of this difference for each data point. Calculating this difference can, again, be done in different ways. One example that we have discussed in class is the squared-loss for linear regression. (Here, the difference between predicted and correct classification is squared so positive and negative differences don't cancel eachother.) \n",
    "$$\\mathcal{L} = \\tfrac{1}{2}\\sum_{n = 1}^N (y_n - \\dot{y}_n)^2$$\n",
    "Here, $n$ denotes the different datapoints, $y_n$ and $\\dot{y}$ represent the correct and the predicted value for that data point respectively. \n",
    "\n",
    "\n",
    "\n",
    "The smaller the outcome of this loss function, the better the Neural Network predicts the data. Therefore, we concentrate on **minimizing the loss function** as a means for **training** the neural network. \n",
    "\n",
    "\n",
    "Training is done with [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). Another word for **gradient** is **derivative**. We use derivatives to update the weights of the neural network to make better predictions. \n",
    "The weights of the $k$-th layer are updated according to the following scheme:\n",
    "$$\n",
    "w_k \\leftarrow w_k - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial w_k} \n",
    "$$\n",
    "* $\\partial f(x)/\\partial x$ means the partial derivative of $f(x)$ with respect to $x$. \n",
    "* Hyperparameter $\\gamma$ is called the *learning rate* (You'll learn more about hyperparameters later. For now, the only thing you'll have to know is that the value of a hyperparameter is set by you.) \n",
    "* Note that $k$ denotes a layer and $n$ denotes a data point.\n",
    "\n",
    "\n",
    "The computation of $\\partial \\mathcal{L}/\\partial w_k$ is done using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_k} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_k}\n",
    "\\frac{\\partial h_k}{\\partial w_k} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_{k+1}}\n",
    "\\frac{\\partial h_{k+1}}{\\partial h_k}\n",
    "\\frac{\\partial h_k}{\\partial w_k} = \\dots\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, for each layer, we can calculate the following expressions: \n",
    "* $h_k = f_k(h_{k-1}, w_k)$ - the forward pass\n",
    "* $\\partial h_{k}/\\partial h_{k-1}$ - the partial derivative of the output with respect to the input\n",
    "* $\\partial h_{k}/\\partial w_k$ - the partial derivative of the output with respect to the parameters\n",
    "\n",
    "\n",
    "This whole process of updating weights by calculating the gradient is called [Backpropagation](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf). Click [here](https://www.youtube.com/watch?v=Ilg3gGewQ5U) for a pretty good video explaining backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dense layer\n",
    "A dense Layer is the basic layer of a neural network. (Other terms for dense layer are fully-connected layer and multiplicative layer.) A dense layer transforms an input matrix of size `(n_objects, d_in)` to a matrix of size `(n_objects, d_out)` (where d stands for dimensions) by performing the following operation:\n",
    "$$\n",
    "H = XW + b\n",
    "$$\n",
    "Here $H$ represents the function of the dense layer, $X$ is the input matrix, $W$ is the weight matrix for this layer and $b$ is the bias. The bias $b$ is a vector. \n",
    "\n",
    "A more detailed version of this function is: \n",
    "$$\n",
    "H_{nk} = \\sum\\limits_{i=1}^{d_{in}} X_{ni}W_{ik} + b_k\n",
    "$$\n",
    "where $n$ denotes again a single data object and $k$ the $k^{th}$ layer.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "You have a neural network of just 1 layer. The inputs are points in a 3D space and you want to classify this point as either $-1$ or $1$. \n",
    "You have $75$ objects in your training set. \n",
    "\n",
    "Therefore, $X$ has shape $75 \\times 3$. $H$ has shape $75 \\times 1$. Weight $W$ of the layer has shape $3 \\times 1$.\n",
    "\n",
    "**NOTE: \"Dense Layer\" is linear. So its mapping is exactly the same as of \"Linear\" function from the previous week but `n_out` is not restricted to 1. We use name \"Dense\" in order to distinquish between these two functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of a dense layer \n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_forward, ['x_input', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll implement a backward pass. As decribed above, this is calculated with the gradient. To calculate the gradient, we'll use the chain rule: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with \n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_grad_input, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of computing the gradient with respect to the input, we'll calculate the gradient with respect to the weights and to the bias: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial W} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial b} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to W parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to W parameter of the layer\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_grad_W, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to b parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to b parameter of the layer\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_grad_b, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer Class\n",
    "\n",
    "Here, we define a basic class for the dense layer. You will use this in the Experiments sections below. You don't need to know how this works; we implement it for you, but it is based on the functions you've written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "    \n",
    "    def get_params(self):\n",
    "        return []\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        #Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(scale=0.01, size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        #initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self, x_input):\n",
    "        self.output = dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = Dense(2, 1)\n",
    "x_input = np.random.random((3, 2))\n",
    "y_output = dense_layer.forward(x_input)\n",
    "print(x_input)\n",
    "print(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ReLU nonlinearity\n",
    "\n",
    "The dense layer, from previous section, is linear. Combinging several linear (dense) layers is always equivalent to a single dense layer. Here is the mathematically proof for this: \n",
    "$$\n",
    "H_1 = XW_1 + b_1\\\\\n",
    "H_2 = H_1W_2 + b_2\\\\\n",
    "H_2 = (XW_1 + b_1)W_2 + b_2 = X(W_1W_2) + (b_1W_2 + b_2) = XW^* + b^*\n",
    "$$\n",
    "\n",
    "\n",
    "For this reason, we also need non-linear layers. Non-linear layers ($f$ in the following) are mostly element-wise and hold the following:\n",
    "$$\n",
    "H_1 = XW_1 + b_1\\\\\n",
    "H_2 = f(H_1)W_2 + b_2\\\\\n",
    "H_2 = f(XW_1 + b_1)W_2 + b_2 \\neq XW^* + b^*\n",
    "$$\n",
    "\n",
    "A popular example of a simple non-linear layer is **ReLU** (Rectified Linear Unit). ReLU doesn't have weights that can be optimized like a dense layer.\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "<img src=\"./src/relu.png\" width=\"500\">\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\Big(\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "\\Big) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next, you will implement the forward pass and backward pass (gradient) for ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of relu layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test forward pass for ReLU, see example above\n",
    "x_input = np.array([[1, -0.5],\n",
    "                    [0.3, 0.1]])\n",
    "\n",
    "print(relu_forward(x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, relu_forward, ['x_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient. \n",
    "        Calculate the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, relu_grad_input, ['x_input', 'grad_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = relu_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Sigmoid nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = blocks.sigmoid_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return blocks.sigmoid_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Sequential model\n",
    "In order to make the work with layers more comfortable, we create `SequentialNN` - a class, which stores all its layers and performs the basic manipulations. Again, this is for the experiments below and you don't need to know how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.training_phase = True\n",
    "        \n",
    "    def set_training_phase(self, is_training=True):\n",
    "        self.training_phase = is_training\n",
    "        for layer in self.layers:\n",
    "            layer.training_phase = is_training\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "            \n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_params())\n",
    "        return params\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            grads.extend(layer.get_params_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the simple neural network. It takes an input of shape `(Any, 10)` and passes it through `Dense(10, 4)`, `ReLU` and `Dense(4, 1)`. The output is a batch of size `(Any, 1)`. \n",
    "```\n",
    "  INPUT\n",
    "    |\n",
    "Dense(10, 4)\n",
    "    |\n",
    "   ReLU\n",
    "    |\n",
    "Dense(4, 1)\n",
    "    |\n",
    "  OUTPUT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SequentialNN(\n",
    "    Dense(10, 4), \n",
    "    ReLU(),\n",
    "    Dense(4, 1),\n",
    "    Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.forward(np.ones([2, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 NLL loss function\n",
    "Here we will define the loss functions. Each loss should be able to compute its value and compute its gradient with respect to the input. We have implemented these functions (e.g. forward, backward) for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(object):\n",
    "    \n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = blocks.nll_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return blocks.nll_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 $L_2$ regularization\n",
    "\n",
    "Loss functions update the weights of your model to improve your predictions. We do this by minimizing the loss function. However, up until now this loss function did not take into account the complexity of your model. Here we mean with complexity the number of parameters that your model stores. We do want to take complexity into account because complex models can perform poorly on test data, while performing excellent on train data. \n",
    "\n",
    "To penalize the complextity of the model, we introduce a regularizer. You'll learn more about regularizers in the lectures, but the general idea is that we take the values of the weights into account with the loss function. High values for weights are indicators of complexity. \n",
    "\n",
    "There are several ways of adding regularization to a model. We will implement [$L_2$ regularization](http://www.deeplearningbook.org/contents/regularization.html) also known as weight decay:\n",
    "\n",
    "The key idea of $L_2$ regularization is to add an extra term to the loss functions:\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\|w\\|^2_2\n",
    "$$\n",
    "\n",
    "The part we added to the loss function is called the regularization function. \n",
    "* $\\lambda$ is named weight decay. It is a hyperparameter that determines the influence of the regularization to the outcome of the loss function. \n",
    "* $\\|w\\|^2_2$ is the squared [euclidian norm](https://en.wikipedia.org/wiki/Euclidean_distance) where $\\|w\\|^2_2 = \\|w_1\\|^2_2 + \\|w_2\\|^2_2 ... \\|w_k\\|^2_2$. \n",
    "This function in more detail becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n",
    "$$\n",
    "\n",
    "Because we use a different loss function, the updating of the weights is also slightly changed: \n",
    "\n",
    "$$\n",
    "w_m \\leftarrow w_m - \\gamma \\frac{\\partial \\mathcal{L}^*}{\\partial w_m}\\\\\n",
    "\\frac{\\partial \\mathcal{L}^*}{\\partial w_m} = \\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\\\\n",
    "w_m \\leftarrow w_m - \\gamma \\Big(\\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\Big)\n",
    "$$\n",
    "\n",
    "Here, you'll implement the computation of $L_2$: \n",
    "$$\n",
    "L_2(\\lambda, [w_1, w_2, \\dots, w_k]) = \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Compute the L2 regularization term\n",
    "    # Arguments\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of different shapes\n",
    "    # Output\n",
    "        sum of the L2 norms of the input weights\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your forward pass below. Your output should be: `108.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the L2 regularizer\n",
    "weight_decay = 2\n",
    "weights = np.array([5,3,7,5,0.5])\n",
    "print(l2_regularizer(weight_decay, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, l2_regularizer, ['weight_decay', 'weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    '''\n",
    "    Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    '''\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * (dw + self.weight_decay * w)\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function from week 2\n",
    "def generate_2_circles(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.hstack([X1,X2]).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split(X, Y, train_ratio=0.7):\n",
    "    size = len(X)\n",
    "    train_size = int(size * train_ratio)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]\n",
    "\n",
    "\n",
    "def plot_model_prediction(prediction_func, X, Y, hard=True):\n",
    "    u_min = X[:, 0].min() - 1\n",
    "    u_max = X[:, 0].max() + 1\n",
    "    v_min = X[:, 1].min() - 1\n",
    "    v_max = X[:, 1].max() + 1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    if hard:\n",
    "        c = c > 0.5\n",
    "    plt.scatter(UV[:,0], UV[:,1], c=c, edgecolors= 'none', alpha=0.15)\n",
    "    plt.scatter(X[:,0], X[:,1], c=Y.ravel(), edgecolors= 'black')\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training the network ##\n",
    "###YOUR CODE FOR DESIGNING THE NETWORK ###\n",
    "model = SequentialNN(\n",
    "    # 2 -> 16 -> 1 With ReLU and Sigmoid where it is required\n",
    ")\n",
    "\n",
    "\n",
    "loss = NLL()\n",
    "weight_decay = 1e-4\n",
    "sgd = SGD(model, lr=0.1, weight_decay=weight_decay)\n",
    "iters = 5000 # Number of times to iterate over all data objects\n",
    "\n",
    "model.set_training_phase(True)\n",
    "\n",
    "for i in range(iters):\n",
    "    # get the predictions\n",
    "    y_pred = model.forward(X_train)\n",
    "    \n",
    "    # compute the loss value + L_2 term\n",
    "    loss_value = loss.forward(y_pred, Y_train) + l2_regularizer(weight_decay, model.get_params())\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        # log the current loss value\n",
    "        print('Step: {}, \\tLoss = {:.2f}'.format(i, loss_value))\n",
    "    \n",
    "    # get the gradient of the loss functions\n",
    "    loss_grad = loss.backward(y_pred, Y_train)\n",
    "\n",
    "    # backprop the gradients\n",
    "    model.backward(X_train, loss_grad)\n",
    "    \n",
    "    # perform the updates\n",
    "    sgd.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(lambda x: model.forward(x), X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutions\n",
    "## 3.1 Matrix convolution\n",
    "\n",
    "\n",
    "There is a way to create a **locally connected** layer which will learn local correlations using a smaller amount of parameters.  \n",
    "This layer is aptly called **Convolutional Layer** and is based on **matrix convolution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A picture is worth a thousand words which is especially true when learning about convolution:\n",
    "![Image convolution](./src/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In image convolution, a **filter**, also called **kernel**, is applied to the source matrix.  \n",
    "Each element from the kernel is multiplied by the corresponding element from the source matrix. The results are summed up and written to the target matrix.\n",
    "\n",
    "In this example, the output matrix has a smaller size than its source\\*. This is because the kernel can not overlap the borders. **Zero padding** can be used to retain the original dimension. It is a simple solution which involves adding a border of zeros to the input.\n",
    "\n",
    "\\* It may seem both matrices have the same size (both are shown with the same number of boxes. In the edges of the right matrix, however, no values are stored. The top-left corner of the right image starts where the $-3$ is placed.\n",
    "\n",
    "The source matrix $X$ is of size $N \\times M$ and the kernel $K$ is of size $(2p+1) \\times (2q +1 )$.  \n",
    "We define $X_{ij} = 0$ for $i > N, i < 1$ and $j > M, j < 1$.  \n",
    "In (other) words: If you try to access a pixel which is out of bounds assume that it is zero.  \n",
    "This is called **zero padding**.\n",
    "\n",
    "Therefore, the convolution of a matrix with a kernel is defined as follows:\n",
    "$$\n",
    "Y = X \\star K \\\\\n",
    "Y_{ij} = \\sum\\limits_{\\alpha=0}^{2p} \\sum\\limits_{\\beta=0}^{2q}\n",
    "K_{\\alpha \\beta} X_{i + \\alpha - p, j+\\beta - q}\n",
    "$$\n",
    "\n",
    "This operation's name depends on the field:\n",
    "* In machine learning: **convolution**\n",
    "* In mathematics: **cross-correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, its time for you to implement matrix convolution.  \n",
    "You can use the example below this code block to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_matrix(matrix, kernel):\n",
    "    \"\"\"Perform the convolution of the matrix \n",
    "        with the kernel using zero padding\n",
    "    # Arguments\n",
    "        matrix: input matrix np.array of size `(N, M)`\n",
    "        kernel: kernel of the convolution \n",
    "            np.array of size `(2p + 1, 2q + 1)`\n",
    "    # Output\n",
    "        the result of the convolution\n",
    "        np.array of size `(N, M)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with the following data:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 3 & 4 \\\\\n",
    "3 & 4 & 5 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2 \\\\\n",
    "\\end{bmatrix} \\quad \n",
    "X \\star K = \n",
    "\\begin{bmatrix}\n",
    "7 & 10 & 3 \\\\\n",
    "10 & 14 & 6 \\\\\n",
    "3 & 6 & 8 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We recreate the example data in Python to perform a local test run.  \n",
    "Don't be confused by [np.eye](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.eye.html). It fills our kernel matrix with ones on the diagonal from top-left to bottom-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, 3, 4],\n",
    "    [3, 4, 5]\n",
    "])\n",
    "\n",
    "K = np.eye(3)\n",
    "K[-1, -1] = 2\n",
    "print(np.zeros(3))\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code block and compare the result with the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv_matrix(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, conv_matrix, ['matrix', 'kernel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Basic kernels\n",
    "\n",
    "Matrix convolution can be used to process an image (think Instagram): blur, shift, detecting edges, and much more.  \n",
    "This [article](http://setosa.io/ev/image-kernels/) (**recommended read**) about image kernels should give you a better understanding of convolutions. It happens to be interactive as well.\n",
    "\n",
    "In convolutional layers, the kernels are learned by training on the dataset. However, there are predefined kernels, for example used on your Instagram photos. Some examples are:\n",
    "\n",
    "**Sharpen Kernel:** \n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0 \n",
    "\\end {bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "**Edge detection filter:**\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & 8 & -1 \\\\\n",
    "-1 & -1 & -1 \n",
    "\\end {bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "**Box blur of size 3:**\n",
    "$$ \\frac{1}{9}\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \n",
    "\\end {bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Let's play with convolutions by manipulating an image of a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = plt.imread('./images/dog.png')\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coloured images would require a 3-dimensional tensor to represent RGB (red, green, and blue).  \n",
    "Therefore, we will convert it to grayscale. This way it can be processed as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rgb_img.mean(axis=2)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's blur the image with [box blur](https://en.wikipedia.org/wiki/Box_blur). It is just a convolution of a matrix with the kernel of size $N \\times N$ of the following form:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N^2}\n",
    "\\begin{bmatrix}\n",
    "1 & \\dots  & 1\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "1 & \\dots  & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Every element of this filter is *one* and we divide the sum by the total amount of elements in the blur filter. You could understand it as taking the average of an image region.\n",
    "\n",
    "**Description:**  \n",
    "Perform the blur of the image.\n",
    "\n",
    "<u>Arguments:</u>\n",
    "* `image` - Input matrix - [np.array](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.array.html) of size `(N, M)`\n",
    "* `box_size` - Size of the blur kernel - `int > 0` the kernel is of size `(box_size, box_size)`\n",
    "\n",
    "<u>Output:</u>  \n",
    "The result of the blur [np.array](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.array.html) of size `(N, M)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_blur(image, box_size):\n",
    "    \"\"\"Perform the blur of the image\n",
    "    # Arguments\n",
    "        image: input matrix - np.array of size `(N, M)`\n",
    "        box_size: the size of the blur kernel - int > 0  \n",
    "            the kernel is of size `(box_size, box_size)`\n",
    "    # Output\n",
    "        the result of the blur\n",
    "            np.array of size `(N, M)`\n",
    "    \"\"\"   \n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your solution before submitting it. Running the following code block should yield this result:\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 4 & 2 \\\\\n",
    "1 & 2 & 1 \n",
    "\\end {bmatrix}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([\n",
    "    [9, 0, 9],\n",
    "    [0, 0, 0],\n",
    "    [9, 0, 9]\n",
    "])\n",
    "\n",
    "print(box_blur(test_image, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, box_blur, ['image', 'box_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's blur the dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_dog = box_blur(img, box_size=3)\n",
    "plt.imshow(blur_dog, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will get the vertical and horizontal gradients. To perform it we calculate the convolution of the image with the following kernels:\n",
    "\n",
    "$$\n",
    "K_h = \n",
    "\\begin{bmatrix}\n",
    "-1 & 0  & 1\\\\\n",
    "\\end{bmatrix} \\quad\n",
    "K_v = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "X_h = X \\star K_h \\quad X_v = X \\star K_v\\\\\n",
    "$$\n",
    "\n",
    "And then we calculate the amplitude of the gradient:\n",
    "\n",
    "$$\n",
    "X_\\text{grad} = \\sqrt{X_h^2 + X_v^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_h = conv_matrix(blur_dog, np.array([[-1, 0, 1]]))\n",
    "dog_v = conv_matrix(blur_dog, np.array([[-1, 0, 1]]).T)\n",
    "dog_grad = np.sqrt(dog_h ** 2 + dog_v ** 2)\n",
    "plt.imshow(dog_grad, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields the edges of our blurred dog. It is not the only way to obtain edges though, there are plenty more:\n",
    "* [Canny edge detection](https://en.wikipedia.org/wiki/Canny_edge_detector)\n",
    "* [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "* [Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you convolve an image with a kernel you obtain a map of responses. The more correlated the patch of an image is with the kernel, the higher the response. Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "# Create the image\n",
    "image = np.pad(pattern, [(12, 12), (10, 14)], mode='constant', constant_values=0)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('original image')\n",
    "plt.show()\n",
    "\n",
    "# Add some noise\n",
    "image = 0.5 * image + 0.5 * np.random.random(image.shape)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('noisy image')\n",
    "plt.show()\n",
    "\n",
    "# Let's find the cross \n",
    "response = conv_matrix(image, pattern)\n",
    "plt.imshow(response, cmap='gray')\n",
    "plt.title('local response')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(response == response.max(), cmap='gray')\n",
    "plt.title('detected position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brightest pixel highlights where the cross is located. We can find the area where the image is locally close to the kernel. This is especially useful for finding different patterns in images such as: eyes, legs, dogs, cats, etc.\n",
    "\n",
    "We defined kernels and applied them to images. But we can also **learn** them by minimizing loss and making the processing as effective as possible. In order to do this, we have to define the **Convolutional Layer** in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Convolutional layer\n",
    "\n",
    "A **Convolutional Layer** works with images. Each image is a 3-dimensional object  $N_{\\text{channels}} \\times H \\times W$.  \n",
    "Here index *\"channels\"* refers to the 3 colors (or 1 for black & white images), $H$ to height, and $W$ to width.  \n",
    "And therefore, the collection of images is 4-dimensional tensor of shape $N_{\\text{objects}} \\times N_{\\text{channels}} \\times H \\times W$.\n",
    "\n",
    "For example, 32 RGB images of size $224 \\times 224$ are represented as a tensor of shape $32 \\times 3 \\times 224 \\times 224$\n",
    "\n",
    "A convolutional layer receives an image as its input. Here is how it works:  \n",
    "The layer has `n_in * n_out` kernels. It is a tensor of size `(n_in, n_out, kernel_h, kernel_w)`  \n",
    "It takes a 4-dimensional tensor of size `n_objects, n_in, H, W` as its input. \n",
    "* `n_objects` is the collection of images. \n",
    "* Each of them has `n_in` channels.\n",
    "* The resolution of the images is `(H, W)`\n",
    "\n",
    "For each of the images the following operation is performed:  \n",
    "* In order to get the 1st output channel, all inputs are convolved with their corresponding kernels.  \n",
    "* Then the results are summed and written to the output channel.  \n",
    "This is our implementation:\n",
    "```python\n",
    "for i in range(n_out):\n",
    "    out_channel = 0.0\n",
    "    for j in range(n_in):\n",
    "        kernel_2d = K[i, j] # Retrieve kernel from the collection of kernels\n",
    "        input_channel = input_image[j] # Get one channel of the input image\n",
    "        out_channel += conv_matrix(input_channel, kernel_2d) # Perform convolution\n",
    "    output_image.append(out_channel) # Append the calculated channel to the output          \n",
    "```\n",
    "\n",
    "We implemented the convolutional layer for you. The implementation of `backward` is based on the idea that convolution could be represented as matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(Layer):\n",
    "    \"\"\"\n",
    "    Convolutional Layer. The implementation is based on \n",
    "        the representation of the convolution as matrix multiplication\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out, filter_size):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.W = np.random.normal(size=(n_out, n_in, filter_size, filter_size))\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        n_obj, n_in, h, w = x_input.shape\n",
    "        n_out = len(self.W)\n",
    "        \n",
    "        self.output = []\n",
    "        \n",
    "        for image in x_input:\n",
    "            output_image = []\n",
    "            for i in range(n_out):\n",
    "                out_channel = 0.0\n",
    "                for j in range(n_in):\n",
    "                    out_channel += conv_matrix(image[j], self.W[i, j])\n",
    "                output_image.append(out_channel)\n",
    "            self.output.append(np.stack(output_image, 0))\n",
    "\n",
    "        self.output = np.stack(self.output, 0)\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "\n",
    "        N, C, H, W = x_input.shape \n",
    "        F, C, HH, WW = self.W.shape\n",
    "        \n",
    "        pad = int((HH - 1) / 2)\n",
    "\n",
    "        self.grad_b = np.sum(grad_output, (0, 2, 3)) \n",
    "\n",
    "        # pad input array\n",
    "        x_padded = np.pad(x_input, ((0,0), (0,0), (pad, pad), (pad, pad)), 'constant')\n",
    "        H_padded, W_padded = x_padded.shape[2], x_padded.shape[3]\n",
    "        # naive implementation of im2col\n",
    "        x_cols = None\n",
    "        for i in range(HH, H_padded + 1):\n",
    "            for j in range(WW, W_padded+1):\n",
    "                for n in range(N):\n",
    "                    field = x_padded[n, :, i-HH:i, j-WW:j].reshape((1,-1))    \n",
    "                    if x_cols is None:\n",
    "                        x_cols = field\n",
    "                    else:\n",
    "                        x_cols = np.vstack((x_cols, field))\n",
    "                        \n",
    "        x_cols = x_cols.T\n",
    "\n",
    "        d_out = grad_output.transpose(1, 2, 3, 0) \n",
    "        dout_cols = d_out.reshape(F, -1) \n",
    "\n",
    "        dw_cols = np.dot(dout_cols, x_cols.T) \n",
    "        self.grad_W = dw_cols.reshape(F, C, HH, WW) \n",
    "\n",
    "        w_cols = self.W.reshape(F, -1) \n",
    "        dx_cols = np.dot(w_cols.T, dout_cols) \n",
    "\n",
    "        dx_padded = np.zeros((N, C, H_padded, W_padded))\n",
    "        idx = 0\n",
    "        for i in range(HH, H_padded + 1):\n",
    "            for j in range(WW, W_padded + 1):\n",
    "                for n in range(N):\n",
    "                    dx_padded[n:n+1, :, i-HH:i, j-WW:j] += dx_cols[:, idx].reshape((1, C, HH, WW))\n",
    "                    idx += 1\n",
    "            dx = dx_padded[:, :, pad:-pad, pad:-pad]\n",
    "        grad_input = dx\n",
    "        return grad_input\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer transforms images with 3 channels into images with 8 channels by convolving them with kernels of size `(3, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = ConvLayer(3, 8, filter_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pooling layer\n",
    "\n",
    "The pooling layer **reduces the size of an image**. \n",
    "\n",
    "In the following figure $2 \\times 2$ pooling is applied on the image which effectively reduces the size by half.  \n",
    "If you look closely, pooling operations have no effect on the depth of an image.\n",
    "![pool](./src/pool.png)\n",
    "\n",
    "There are several types of pooling operations but the most common one is **max pooling**. \n",
    "\n",
    "During a max pooling operation, the image is split into **windows** (or **filters**) and then the maximum of each window is used as the output.\n",
    "\n",
    "![maxpool](./src/maxpool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_forward(x_input):\n",
    "    \"\"\"Perform max pooling operation with 2x2 window\n",
    "    # Arguments\n",
    "        x_input: np.array of size (2 * W, 2 * H)\n",
    "    # Output\n",
    "        output: np.array of size (W, H)\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    ################# \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you can use example data to test your solution:  \n",
    "**Image:**\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 2 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "3 & 2 & 1 & 0 \\\\\n",
    "1 & 2 & 3 & 4\n",
    "\\end {bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "**Output:**\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "6 & 8 \\\\\n",
    "3 & 4\n",
    "\\end {bmatrix}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([\n",
    "    [1, 1, 2, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [3, 2, 1, 0],\n",
    "    [1, 2, 3, 4]\n",
    "])\n",
    "\n",
    "print(maxpool_forward(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, maxpool_forward, ['x_input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the gradient calculation.  \n",
    "It is not overly complicated; reading the code should help you to understand the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_grad_input(x_input, grad_output):\n",
    "    \"\"\"Calculate partial derivative of the loss with respect to the input\n",
    "    # Arguments\n",
    "        x_input: np.array of size (2 * W, 2 * H)\n",
    "        grad_output: partial derivative of the loss \n",
    "            with respect to the output \n",
    "            np.array of size (W, H)\n",
    "    # Output\n",
    "        output: partial derivative of the loss \n",
    "            with respect to the input\n",
    "            np.array of size (2 * W, 2 * H) \n",
    "    \"\"\"\n",
    "    height, width = x_input.shape\n",
    "    # create the array of zeros of the required size\n",
    "    grad_input = np.zeros(x_input.shape)\n",
    "    \n",
    "    # let's put 1 if the element with this position \n",
    "    # is maximal in the window\n",
    "    for i in range(0, height, 2):\n",
    "        for j in range(0, width, 2):\n",
    "            window = x_input[i:i+2, j:j+2]\n",
    "            i_max, j_max = np.unravel_index(np.argmax(window), (2, 2))\n",
    "            grad_input[i + i_max, j + j_max] = 1\n",
    "            \n",
    "    # put corresponding gradient instead of 1       \n",
    "    grad_input = grad_input.ravel()\n",
    "    grad_input[grad_input == 1] = grad_output.ravel()\n",
    "    grad_input = grad_input.reshape(x_input.shape)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following up is the full implementation of the **MaxPool Layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2x2(Layer):\n",
    "    \n",
    "    def forward(self, x_input):\n",
    "        n_obj, n_ch, h, w = x_input.shape\n",
    "        self.output = np.zeros((n_obj, n_ch, h // 2, w // 2))\n",
    "        for i in range(n_obj):\n",
    "            for j in range(n_ch):\n",
    "                self.output[i, j] = maxpool_forward(x_input[i, j])\n",
    "        return self.output \n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        n_obj, n_ch, _, _ = x_input.shape\n",
    "        grad_input = np.zeros_like(x_input)\n",
    "        for i in range(n_obj):\n",
    "            for j in range(n_ch):\n",
    "                grad_input[i, j] = maxpool_grad_input(x_input[i, j], grad_output[i, j])\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Flatten\n",
    "\n",
    "Convolutional neural networks are better at image processing than fully connected neural networks (dense networks). We will combine convolutional layers, which deal with 4-dimensional tensors, with dense layers, which work with matrices.  \n",
    "In order to bridge the gap between convolutional layers and dense layers we will implement the **Flatten Layer**.\n",
    "\n",
    "The Flatten layer receives a 4-dimensional tensor of size `(n_obj, n_channels, h, w)` as its input and reshapes it into a 2-dimensional tensor (matrix) of size `(n_obj, n_channels * h * w)`.  \n",
    "\n",
    "The backward pass of this layer is pretty straightforward. Remember that we don't actually change any values; we merely reshape inputs.\n",
    "\n",
    "**Please implement `flatten_forward` and `flatten_grad_input` functions using [np.reshape](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_forward(x_input):\n",
    "    \"\"\"Perform the reshaping of the tensor of size `(K, L, M, N)` \n",
    "        to the tensor of size `(K, L*M*N)`\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(K, L, M, N)`\n",
    "    # Output\n",
    "        output: np.array of size `(K, L*M*N)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use test data and compare the final shape. It should be `(100, 768)` for the following example.  \n",
    "Please ignore the use of [np.zeros](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.zeros.html) in this case. We are just interested in transforming shapes.  \n",
    "**Be aware:** This test will fail if you do not return an array like object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.zeros((100, 3, 16, 16))\n",
    "\n",
    "print(flatten_forward(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, flatten_forward, ['x_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_grad_input(x_input, grad_output):\n",
    "    \"\"\"Calculate partial derivative of the loss with respect to the input\n",
    "    # Arguments\n",
    "        x_input: partial derivative of the loss \n",
    "            with respect to the output\n",
    "            np.array of size `(K, L*M*N)`\n",
    "    # Output\n",
    "        output: partial derivative of the loss \n",
    "            with respect to the input\n",
    "            np.array of size `(K, L, M, N)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, flatten_grad_input, ['x_input', 'grad_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the, pretty self-explanatory, implemention of the **Flatten Layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    \n",
    "    def forward(self, x_input):\n",
    "        self.output = flatten_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        output = flatten_grad_input(x_input, grad_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Image Experiments\n",
    "\n",
    "This chapter focuses on conducting several experiments. We will train our neural networks with **mini-batches**. Mini-batches are small portions of our dataset, all mini-batches together should form the original dataset again. With our mini-batches in place we will feed these one-by-one to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def iterate_minibatches(x, y, batch_size=16, verbose=True):\n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i, start_idx in enumerate(range(0, len(x) - batch_size + 1, batch_size)):\n",
    "        if verbose:\n",
    "            print('\\rBatch: {}/{}'.format(i + 1, len(x) // batch_size), end='')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        yield x[excerpt], y[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data. Please [download](http://yann.lecun.com/exdb/mnist/) it first.\n",
    "\n",
    "If you get an error with loading the data, chances are you'll need to unpack the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(load_mnist(dataset='training', path='.'))\n",
    "train\n",
    "train_images = np.array([im[1] for im in train])\n",
    "train_targets = np.array([im[0] for im in train])\n",
    "# We will train a 0 vs. 1 classifier\n",
    "x_train = train_images[train_targets < 2][:1000]\n",
    "y_train = train_targets[train_targets < 2][:1000]\n",
    "\n",
    "y_train = y_train \n",
    "y_train = y_train.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just loaded the MNIST dataset. This dataset consists of gray-scale (so a single channel) images of size `28x28`. These images are represented by the RGB color model. This color model representes a color by 3 integers that range from 0 to 255, or in the case of gray-scale images this is a single integer. This means that each picture in the MNIST dataset is represented by 784 pixels with a value ranging from 0 to 255. This is how a single image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make divergence to an optimum easier, we will normalize the images to have values between 0 and 1. Then, by reshaping, we will add the dimensions for the channel which, for simplicity, was removed by the creators of this dataset. As you can see, this doesn't change anything in how the image looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_train = x_train.reshape((-1, 1, 28, 28))\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train a simple convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn():\n",
    "    nn = SequentialNN(\n",
    "        ConvLayer(1, 2, filter_size=3), # The output is of size [N_obj 2 28 28]\n",
    "        ReLU(), # The output is of size [N_obj 2 28 28]\n",
    "        MaxPool2x2(), # The output is of size [N_obj 2 14 14]\n",
    "        ConvLayer(2, 4, filter_size=3), # The output is of size [N_obj 4 14 14]\n",
    "        ReLU(), # The output is of size [N_obj 4 14 14]\n",
    "        MaxPool2x2(), # The output is of size [N_obj 4 7 7]\n",
    "        FlattenLayer(),  # The output is of size [N_obj 196]\n",
    "        Dense(4 * 7 * 7, 8),\n",
    "        ReLU(),\n",
    "        Dense(8, 1),\n",
    "        Sigmoid()\n",
    "    )\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = get_cnn()\n",
    "loss = NLL()\n",
    "optimizer = SGD(nn, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will train for about 5 minutes\n",
    "num_epochs = 5 \n",
    "batch_size = 32\n",
    "# We will store the results here\n",
    "history = {'loss': [], 'accuracy': []}\n",
    "\n",
    "# `num_epochs` represents the number of iterations\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "    \n",
    "    # We perform iteration a one-by-one iteration of the mini-batches\n",
    "    for x_batch, y_batch in iterate_minibatches(x_train, y_train, batch_size):\n",
    "        # Predict the target value\n",
    "        y_pred = nn.forward(x_batch)\n",
    "        # Compute the gradient of the loss\n",
    "        loss_grad = loss.backward(y_pred, y_batch)\n",
    "        # Perform backwards pass\n",
    "        nn.backward(x_batch, loss_grad)\n",
    "        # Update the params\n",
    "        optimizer.update_params()\n",
    "        \n",
    "        # Save loss and accuracy values\n",
    "        history['loss'].append(loss.forward(y_pred, y_batch))\n",
    "        prediction_is_correct = (y_pred > 0.5) == (y_batch > 0.5)\n",
    "        history['accuracy'].append(np.mean(prediction_is_correct))\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results to get a better insight\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax_1 = plt.subplot()\n",
    "ax_1.plot(history['loss'], c='g', lw=2, label='train loss')\n",
    "ax_1.set_ylabel('loss', fontsize=16)\n",
    "ax_1.set_xlabel('#batches', fontsize=16)\n",
    "\n",
    "ax_2 = plt.twinx(ax_1)\n",
    "ax_2.plot(history['accuracy'], lw=3, label='train accuracy')\n",
    "ax_2.set_ylabel('accuracy', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things you could try:**  \n",
    "Train the model with a different `batch_size`:\n",
    "* What would happen with `batch_size=1`?\n",
    "* What would happen with `batch_size=1000`?\n",
    "* Does the speed of the computation depend on this parameter? If so, why?\n",
    "\n",
    "Train the model with a different number of `num_epochs`:\n",
    "* What would happen with `num_epochs=1`?\n",
    "* What would happen with `num_epochs=1000`?\n",
    "* How does it affect computation time, resource strain, and accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the activations of the intermediate layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_images = x_batch[:2]\n",
    "_ = nn.forward(viz_images)\n",
    "\n",
    "activations = {\n",
    "    'conv_1': nn.layers[0].output,\n",
    "    'relu_1': nn.layers[1].output,\n",
    "    'pool_1': nn.layers[2].output,\n",
    "    'conv_2': nn.layers[3].output,\n",
    "    'relu_2': nn.layers[4].output,\n",
    "    'pool_2': nn.layers[5].output,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(4, 8))\n",
    "\n",
    "ax1.imshow(viz_images[0, 0], cmap=plt.cm.gray_r)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax2.imshow(viz_images[1, 0], cmap=plt.cm.gray_r)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Conv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations['conv_1'][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Channel {}'.format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of ReLU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations['relu_1'][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Channel {}'.format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of MaxPooling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations['pool_1'][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Channel {}'.format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Conv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations['conv_2'][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Channel {}'.format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of ReLU 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations['relu_2'][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Channel {}'.format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of MaxPooling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations['pool_2'][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Channel {}'.format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we go deeper and deeper, images become less locally-correlated (the dependance between two neighbours decreases) and more semantically loaded.  \n",
    "Each convoluted pixel stores more and more useful information about the object.  \n",
    "In the end, this will be anaylzed using several **Dense Layers**.\n",
    "\n",
    "**Things you could try:**\n",
    "* Change the architecture of the neural network\n",
    "* Vary the number of kernels\n",
    "* Vary the size of the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
