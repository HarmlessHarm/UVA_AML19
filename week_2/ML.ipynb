{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "The goal of this assignment is to supply you with machine learning models and algorithms. In this notebook, we will cover linear and nonlinear models, the concept of loss functions and some optimization techniques. All mathematical operations should be implemented in **NumPy** only. \n",
    "\n",
    "\n",
    "## Table of contents\n",
    "* [1. Logistic Regression](#1.-Logistic-Regression)\n",
    "    * [1.1 Linear Mapping](#1.1-Linear-Mapping)\n",
    "    * [1.2 Sigmoid](#1.2-Sigmoid)\n",
    "    * [1.3 Negative Log Likelihood](#1.3-Negative-Log-Likelihood)\n",
    "    * [1.4 Model](#1.4-Model)\n",
    "    * [1.5 Simple Experiment](#1.5-Simple-Experiment)\n",
    "* [2. Decision Tree](#2.-Decision-Tree)\n",
    "    * [2.1 Gini Index & Data Split](#2.1-Gini-Index-&-Data-Split)\n",
    "    * [2.2 Terminal Node](#2.2-Terminal-Node)\n",
    "    * [2.3 Build the Decision Tree](#2.3-Build-the-Decision-Tree)\n",
    "* [3. Experiments](#3.-Experiments)\n",
    "    * [3.1 Decision Tree for Heart Disease Prediction](#3.1-Decision-Tree-for-Heart-Disease-Prediction) \n",
    "    * [3.2 Logistic Regression for Heart Disease Prediction](#3.2-Logistic-Regression-for-Heart-Disease-Prediction)\n",
    "\n",
    "### Note\n",
    "Some of the concepts below have not (yet) been discussed during the lecture. These will be discussed further during the next lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you begin\n",
    "\n",
    "To check whether the code you've written is correct, we'll use **automark**. For this, we created for each of you an account with the username being your student number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = 'Your Username'\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far all your tests are 'not attempted'. At the end of this notebook you'll need to have completed all test. The output of `am.get_progress(username)` should at least match the example below. However, we encourage you to take a shot at the 'not attempted' tests!\n",
    "\n",
    "```\n",
    "---------------------------------------------\n",
    "| Your name / student number                |\n",
    "| your_email@your_domain.whatever           |\n",
    "---------------------------------------------\n",
    "| linear_forward           | not attempted  |\n",
    "| linear_grad_W            | not attempted  |\n",
    "| linear_grad_b            | not attempted  |\n",
    "| nll_forward              | not attempted  |\n",
    "| nll_grad_input           | not attempted  |\n",
    "| sigmoid_forward          | not attempted  |\n",
    "| sigmoid_grad_input       | not attempted  |\n",
    "| tree_data_split_left     | not attempted  |\n",
    "| tree_data_split_right    | not attempted  |\n",
    "| tree_gini_index          | not attempted  |\n",
    "| tree_to_terminal         | not attempted  |\n",
    "---------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division # You don't need to know what this is. \n",
    "import numpy as np # this imports numpy, which is used for vector- and matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes use of **classes** and their **instances** that we have already implemented for you. It allows us to write less code and make it more readable. If you are interested in it, here are some useful links:\n",
    "* The official [documentation](https://docs.python.org/3/tutorial/classes.html) \n",
    "* Video by *sentdex*: [Object Oriented Programming Introduction](https://www.youtube.com/watch?v=ekA6hvk-8H8)\n",
    "* Antipatterns in OOP: [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "We start with a very simple algorithm called **Logistic Regression**. It is a generalized linear model for 2-class classification.\n",
    "It can be generalized to the case of many classes and to non-linear cases as well. However, here we consider only the simplest case. \n",
    "\n",
    "Let us consider a data with 2 classes. Class 0 and class 1. For a given test sample, logistic regression returns a value from $[0, 1]$ which is interpreted as a probability of belonging to class 1. The set of points for which the prediction is $0.5$ is called a *decision boundary*. It is a line on a plane or a hyper-plane in a space.\n",
    "\n",
    "![](https://nlpforhackers.io/wp-content/uploads/2018/07/Linear-Separability-610x610.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has two trainable parameters: a weight $W$ and a bias $b$. For a vector of features $X$, the prediction of logistic regression is given by\n",
    "\n",
    "$$\n",
    "f(X) = \\frac{1}{1 + \\exp(-[XW + b])} = \\sigma(h(X))\n",
    "$$\n",
    "where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ and $h(X)=XW + b$.\n",
    "\n",
    "Parameters $W$ and $b$ are fitted by maximizing the log-likelihood (or minimizing the negative log-likelihood) of the model on the training data. For a training subset $\\{X_j, Y_j\\}_{j=1}^N$ the normalized negative log likelihood (NLL) is given by \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N}\\sum_j \\log\\Big[ f(X_j)^{Y_j} \\cdot (1-f(X_j))^{1-Y_j}\\Big]\n",
    "= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f(X_j) + (1-Y_j)\\log(1-f(X_j))\\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways of fitting this model. In this assignment we consider Logistic Regression as a one-layer neural network. We use the following algorithm for the **forward** pass:\n",
    "\n",
    "1. Linear mapping: $h=XW + b$\n",
    "2. Sigmoid activation function: $f=\\sigma(h)$\n",
    "3. Calculation of NLL: $\\mathcal{L} = -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f_j + (1-Y_j)\\log(1-f_j)\\Big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit $W$ and $b$ we perform Gradient Descent ([GD](https://en.wikipedia.org/wiki/Gradient_descent)). We choose a small learning rate $\\gamma$ and after each computation of forward pass, we update the parameters \n",
    "\n",
    "$$W_{\\text{new}} = W_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial W}$$\n",
    "\n",
    "$$b_{\\text{new}} = b_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial b}$$\n",
    "\n",
    "We use Backpropagation method ([BP](https://en.wikipedia.org/wiki/Backpropagation)) to calculate the partial derivatives of the loss function with respect to the parameters of the model.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial W} =\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial b} =\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Linear Mapping\n",
    "First of all, you need to implement the forward pass of a linear mapping:\n",
    "$$\n",
    "h(X) = XW +b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: here we use `n_out` as the dimensionality of the output. For logisitc regression `n_out = 1`. However, we will work with cases of `n_out > 1` in next assignments. You will **pass** the current assignment even if your implementation works only in case `n_out = 1`. If your implementation works for the cases of `n_out > 1` then you will not have to modify your method next week. All **numpy** operations are generic. It is recommended to use numpy when is it possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of the linear function - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of the linear function \n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your first function. We set the matrices $X, W, b$:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "W = \\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "b = \\begin{bmatrix}\n",
    "3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And then compute \n",
    "$$\n",
    "XW = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-4 \\\\\n",
    "6 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "XW + b = \n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "-1 \\\\\n",
    "9 \\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[1, -1],\n",
    "                   [-1, 0],\n",
    "                   [1, 1]])\n",
    "\n",
    "W_test = np.array([[4],\n",
    "                   [2]])\n",
    "\n",
    "b_test = np.array([3])\n",
    "\n",
    "h_test = linear_forward(X_test, W_test, b_test)\n",
    "print(h_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, linear_forward, ['x_input', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the calculation of the partial derivative of the loss function with respect to the parameters of the model. As this expressions are used for the updates of the parameters, we refer to them as gradients.\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial W} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial b} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to W parameter of the function\n",
    "        dL / dW = (dL / dh) * (dh / dW)\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer (dL / dh)\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to W parameter of the function\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, linear_grad_W, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to b parameter of the function\n",
    "        dL / db = (dL / dh) * (dh / db)\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the linear function (dL / dh)\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to b parameter of the linear function\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, linear_grad_b, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Sigmoid\n",
    "$$\n",
    "f = \\sigma(h) = \\frac{1}{1 + e^{-h}} \n",
    "$$\n",
    "\n",
    "Sigmoid function is applied element-wise. It does not change the dimensionality of the tensor and its implementation is shape-agnostic in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(x_input):\n",
    "    \"\"\"sigmoid nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of relu layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, sigmoid_forward, ['x_input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the calculation of the partial derivative of the loss function with respect to the input of sigmoid. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial f}\n",
    "\\frac{\\partial f}{\\partial h} \n",
    "$$\n",
    "\n",
    "Tensor $\\frac{\\partial \\mathcal{L}}{\\partial f}$ comes from the loss function. Let's calculate $\\frac{\\partial f}{\\partial h}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial h} = \n",
    "\\frac{\\partial \\sigma(h)}{\\partial h} =\n",
    "\\frac{\\partial}{\\partial h} \\Big(\\frac{1}{1 + e^{-h}}\\Big)\n",
    "= \\frac{e^{-h}}{(1 + e^{-h})^2}\n",
    "= \\frac{1}{1 + e^{-h}} \\frac{e^{-h}}{1 + e^{-h}}\n",
    "= f(h) (1 - f(h))\n",
    "$$\n",
    "\n",
    "Therefore, in order to calculate the gradient of the loss with respect to the input of sigmoid function you need \n",
    "to \n",
    "1. calculate $f(h) (1 - f(h))$ \n",
    "2. multiply it element-wise by $\\frac{\\partial \\mathcal{L}}{\\partial f}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad_input(x_input, grad_output):\n",
    "    \"\"\"sigmoid nonlinearity gradient. \n",
    "        Calculate the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_in)` \n",
    "            dL / df\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to the input of the function\n",
    "        np.array of size `(n_objects, n_in)` \n",
    "        dL / dh\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, sigmoid_grad_input, ['x_input', 'grad_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L} \n",
    "= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log \\dot{Y}_j + (1-Y_j)\\log(1-\\dot{Y}_j)\\Big]\n",
    "$$\n",
    "\n",
    "Here $N$ is the number of objects. $Y_j$ is the real label of an object and $\\dot{Y}_j$ is the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of NLL\n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects, 1)`\n",
    "        target_true: ground truth - np.array of size `(n_objects, 1)`\n",
    "    # Output\n",
    "        the value of NLL for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, nll_forward, ['target_pred', 'target_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to calculate the partial derivative of NLL with with respect to its input.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_0} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_N}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's do it step-by-step\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_0} \n",
    "&= \\frac{\\partial}{\\partial \\dot{Y}_0} \\Big(-\\frac{1}{N}\\sum_j \\Big[ Y_j\\log \\dot{Y}_j + (1-Y_j)\\log(1-\\dot{Y}_j)\\Big]\\Big) \\\\\n",
    "&= -\\frac{1}{N} \\frac{\\partial}{\\partial \\dot{Y}_0} \\Big(Y_0\\log \\dot{Y}_0 + (1-Y_0)\\log(1-\\dot{Y}_0)\\Big) \\\\\n",
    "&= -\\frac{1}{N} \\Big(\\frac{Y_0}{\\dot{Y}_0} - \\frac{1-Y_0}{1-\\dot{Y}_0}\\Big)\n",
    "= \\frac{1}{N} \\frac{\\dot{Y}_0 - Y_0}{\\dot{Y}_0 (1 - \\dot{Y}_0)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "And for the other components it can be done in exactly the same way. So the result is the vector where each component is given by \n",
    "$$\\frac{1}{N} \\frac{\\dot{Y}_j - Y_j}{\\dot{Y}_j (1 - \\dot{Y}_j)}$$\n",
    "\n",
    "Or if we assume all multiplications and divisions to be done element-wise the output can be calculated as\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}} = \\frac{1}{N} \\frac{\\dot{Y} - Y}{\\dot{Y} (1 - \\dot{Y})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative of NLL\n",
    "        with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects, 1)`\n",
    "        target_true: ground truth - np.array of size `(n_objects, 1)`\n",
    "    # Output\n",
    "        the partial derivative \n",
    "        of NLL with respect to its input\n",
    "        np.array of size `(n_objects, 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################  \n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, nll_grad_input, ['target_pred', 'target_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model\n",
    "\n",
    "Here we provide a model for your. It consist of the function which you have implmeneted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogsticRegressionGD(object):\n",
    "    \n",
    "    def __init__(self, n_in, lr=0.05):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.b = np.zeros(1, )\n",
    "        self.W = np.random.randn(n_in, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.h = linear_forward(x, self.W, self.b)\n",
    "        y = sigmoid_forward(self.h)\n",
    "        return y\n",
    "    \n",
    "    def update_params(self, x, nll_grad):\n",
    "        # compute gradients\n",
    "        grad_h = sigmoid_grad_input(self.h, nll_grad)\n",
    "        grad_W = linear_grad_W(x, grad_h, self.W, self.b)\n",
    "        grad_b = linear_grad_b(x, grad_h, self.W, self.b)\n",
    "        # update params\n",
    "        self.W = self.W - self.lr * grad_W\n",
    "        self.b = self.b - self.lr * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Simple Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "def generate_2_circles(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.hstack([X1,X2]).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def generate_2_gaussians(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = np.random.normal(loc=[1, 2], scale=[2.5, 0.9], size=(N, 2))\n",
    "    X1 = X1.dot(np.array([[0.7, -0.7], [0.7, 0.7]]))\n",
    "    X2 = np.random.normal(loc=[-2, 0], scale=[1, 1.5], size=(N, 2))\n",
    "    X2 = X2.dot(np.array([[0.7, 0.7], [-0.7, 0.7]]))\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.vstack([X1,X2])\n",
    "    return X, Y\n",
    "\n",
    "def split(X, Y, train_ratio=0.7):\n",
    "    size = len(X)\n",
    "    train_size = int(size * train_ratio)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "\n",
    "X, Y = generate_2_circles()\n",
    "ax1.scatter(X[:,0], X[:,1], c=Y.ravel(), edgecolors= 'none')\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "\n",
    "X, Y = generate_2_gaussians()\n",
    "ax2.scatter(X[:,0], X[:,1], c=Y.ravel(), edgecolors= 'none')\n",
    "ax2.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_gaussians(), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train our model\n",
    "model = LogsticRegressionGD(2, 0.05)\n",
    "\n",
    "for step in range(30):\n",
    "    Y_pred = model.forward(X_train)\n",
    "    \n",
    "    loss_value = nll_forward(Y_pred, Y_train)\n",
    "    accuracy = ((Y_pred > 0.5) == Y_train).mean()\n",
    "    print('Step: {} \\t Loss: {:.3f} \\t Acc: {:.1f}%'.format(step, loss_value, accuracy * 100))\n",
    "    \n",
    "    loss_grad = nll_grad_input(Y_pred, Y_train)\n",
    "    model.update_params(X_train, loss_grad)\n",
    "\n",
    "    \n",
    "print('\\n\\nTesting...')\n",
    "Y_test_pred = model.forward(X_test)\n",
    "test_accuracy = ((Y_test_pred > 0.5) == Y_test).mean()\n",
    "print('Acc: {:.1f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_prediction(prediction_func, X, Y, hard=True):\n",
    "    u_min = X[:, 0].min()-1\n",
    "    u_max = X[:, 0].max()+1\n",
    "    v_min = X[:, 1].min()-1\n",
    "    v_max = X[:, 1].max()+1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    if hard:\n",
    "        c = c > 0.5\n",
    "    plt.scatter(UV[:,0], UV[:,1], c=c, edgecolors= 'none', alpha=0.15)\n",
    "    plt.scatter(X[:,0], X[:,1], c=Y.ravel(), edgecolors= 'black')\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_prediction(lambda x: model.forward(x), X_train, Y_train, False)\n",
    "\n",
    "plot_model_prediction(lambda x: model.forward(x), X_train, Y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the same experiment on 2 circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Tree\n",
    "The next model we look at is called **Decision Tree**. This type of model is non-parametric, meaning in contrast to **Logistic Regression** we do not have any parameters here that need to be trained.\n",
    "\n",
    "Let us consider a simple binary decision tree for deciding on the two classes of \"creditable\" and \"Not creditable\".\n",
    "\n",
    "![](https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/creditdecisiontree.png)\n",
    "\n",
    "Each node, except the leafs, asks a question about the the client in question. A decision is made by going from the root node to a leaf node, while considering the clients situation. The situation of the client, in this case, is fully described by the features:\n",
    "1. Checking account balance\n",
    "2. Duration of requested credit\n",
    "3. Payment status of previous loan\n",
    "4. Length of current employment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a decision tree we need training data. To carry on the previous example: we need a number of clients for which we know the properties 1.-4. and their creditability.\n",
    "The process of building a decision tree starts with the root node and involves the following steps:\n",
    "1. Choose a splitting criteria and add it to the current node.\n",
    "2. Split the dataset at the current node into those that fullfil the criteria and those that do not.\n",
    "3. Add a child node for each data split.\n",
    "4. For each child node decide on either A. or B.:\n",
    "    1. Repeat from 1. step\n",
    "    2. Make it a leaf node: The predicted class label is decided by the majority vote over the training data in the current split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gini Index & Data Split\n",
    "Deciding on how to split your training data at each node is dominated by the following two criterias:\n",
    "1. Does the rule help me make a final decision?\n",
    "2. Is the rule general enough such that it applies not only to my training data, but also to new unseen examples?\n",
    "\n",
    "When considering our previous example, splitting the clients by their handedness would not help us deciding on their creditability. Knowning if a rule will generalize is usually a hard call to make, but in practice we rely on [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) principle. Thus the less rules we use, the better we believe it to generalize to previously unseen examples.\n",
    "\n",
    "One way to measure the quality of a rule is by the [**Gini Index**](https://en.wikipedia.org/wiki/Gini_coefficient).\n",
    "Since we only consider binary classification, it is calculated by:\n",
    "$$\n",
    "Gini = \\sum_{n\\in\\{L,R\\}}\\frac{|S_n|}{|S|}\\left( 1 - \\sum_{c \\in C} p_{S_n}(c)^2\\right)\\\\\n",
    "p_{S_n}(c) = \\frac{|\\{\\mathbf{x}_{i}\\in \\mathbf{X}|y_{i} = c, i \\in S_n\\}|}{|S_n|}, n \\in \\{L, R\\}\n",
    "$$\n",
    "with $|C|=2$ being your set of class labels and $S_L$ and $S_R$ the two splits determined by the splitting criteria.\n",
    "The lower the gini score, the better the split. In the extreme case, where all class labels are the same in each split respectively, the gini index takes the value of $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_gini_index(Y_left, Y_right, classes):\n",
    "    \"\"\"Compute the Gini Index.\n",
    "    # Arguments\n",
    "        Y_left: class labels of the data left set\n",
    "            np.array of size `(n_objects, 1)`\n",
    "        Y_right: class labels of the data right set\n",
    "            np.array of size `(n_objects, 1)`\n",
    "        classes: list of all class values\n",
    "    # Output\n",
    "        gini: scalar `float`\n",
    "    \"\"\"\n",
    "    gini = 0.0\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, tree_gini_index, ['Y_left', 'Y_right', 'classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each node in the tree, the data is split according to a split criterion and each split is passed onto the left/right child respectively.\n",
    "Implement the following function to return all rows in `X` and `Y` such that the left child gets all examples that are less than the split value and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_split_data_left(X, Y, feature_index, split_value):\n",
    "    \"\"\"Split the data `X` and `Y`, at the feature indexed by `feature_index`.\n",
    "    If the value is less than `split_value` then return it as part of the left group.\n",
    "    \n",
    "    # Arguments\n",
    "        X: np.array of size `(n_objects, n_in)`\n",
    "        Y: np.array of size `(n_objects, 1)`\n",
    "        feature_index: index of the feature to split at \n",
    "        split_value: value to split between\n",
    "    # Output\n",
    "        (XY_left): np.array of size `(n_objects_left, n_in + 1)`\n",
    "    \"\"\"\n",
    "    X_left, Y_left = None, None\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    XY_left = np.concatenate([X_left, Y_left], axis=-1)\n",
    "    return XY_left\n",
    "\n",
    "\n",
    "def tree_split_data_right(X, Y, feature_index, split_value):\n",
    "    \"\"\"Split the data `X` and `Y`, at the feature indexed by `feature_index`.\n",
    "    If the value is greater or equal than `split_value` then return it as part of the right group.\n",
    "    \n",
    "    # Arguments\n",
    "        X: np.array of size `(n_objects, n_in)`\n",
    "        Y: np.array of size `(n_objects, 1)`\n",
    "        feature_index: index of the feature to split at\n",
    "        split_value: value to split between\n",
    "    # Output\n",
    "        (XY_left): np.array of size `(n_objects_left, n_in + 1)`\n",
    "    \"\"\"\n",
    "    X_right, Y_right = None, None\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    XY_right = np.concatenate([X_right, Y_right], axis=-1)\n",
    "    return XY_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, tree_split_data_left, ['X', 'Y', 'feature_index', 'split_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, tree_split_data_right, ['X', 'Y', 'feature_index', 'split_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the split rule with the lowest gini score, we brute-force search over all features and values to split by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_best_split(X, Y):\n",
    "    class_values = list(set(Y.flatten().tolist()))\n",
    "    r_index, r_value, r_score =  float(\"inf\"),  float(\"inf\"), float(\"inf\")\n",
    "    r_XY_left, r_XY_right = (X,Y), (X,Y)\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        for row in X:\n",
    "            XY_left = tree_split_data_left(X, Y, feature_index, row[feature_index])\n",
    "            XY_right = tree_split_data_right(X, Y, feature_index, row[feature_index])\n",
    "            XY_left, XY_right = (XY_left[:,:-1], XY_left[:,-1:]), (XY_right[:,:-1], XY_right[:,-1:])\n",
    "            gini = tree_gini_index(XY_left[1], XY_right[1], class_values)\n",
    "            if gini < r_score:\n",
    "                r_index, r_value, r_score = feature_index, row[feature_index], gini\n",
    "                r_XY_left, r_XY_right = XY_left, XY_right\n",
    "    return {'index':r_index, 'value':r_value, 'XY_left': r_XY_left, 'XY_right':r_XY_right}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Terminal Node\n",
    "The leaf nodes predict the label of an unseen example, by taking a majority vote over all training class labels in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_terminal(Y):\n",
    "    \"\"\"The most frequent class label, out of the data points belonging to the leaf node,\n",
    "    is selected as the predicted class.\n",
    "    \n",
    "    # Arguments\n",
    "        Y: np.array of size `(n_objects)`\n",
    "        \n",
    "    # Output\n",
    "        label: most frequent label of `Y.dtype`\n",
    "    \"\"\"\n",
    "    label = None\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, tree_to_terminal, ['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Build the Decision Tree\n",
    "Now we recursively build the decision tree, by greedily splitting the data at each node according to the gini index.\n",
    "To prevent the model from overfitting, we transform a node into a terminal/leaf node, if:\n",
    "1. a maximum depth is reached.\n",
    "2. the node does not reach a minimum number of training samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_recursive_split(X, Y, node, max_depth, min_size, depth):\n",
    "    XY_left, XY_right = node['XY_left'], node['XY_right']\n",
    "    del(node['XY_left'])\n",
    "    del(node['XY_right'])\n",
    "    # check for a no split\n",
    "    if XY_left[0].size <= 0 or XY_right[0].size <= 0:\n",
    "        node['left_child'] = node['right_child'] = tree_to_terminal(np.concatenate((XY_left[1], XY_right[1])))\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left_child'], node['right_child'] = tree_to_terminal(XY_left[1]), tree_to_terminal(XY_right[1])\n",
    "        return\n",
    "    # process left child\n",
    "    if XY_left[0].shape[0] <= min_size:\n",
    "        node['left_child'] = tree_to_terminal(XY_left[1])\n",
    "    else:\n",
    "        node['left_child'] = tree_best_split(*XY_left)\n",
    "        tree_recursive_split(X, Y, node['left_child'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if XY_right[0].shape[0] <= min_size:\n",
    "        node['right_child'] = tree_to_terminal(XY_right[1])\n",
    "    else:\n",
    "        node['right_child'] = tree_best_split(*XY_right)\n",
    "        tree_recursive_split(X, Y, node['right_child'], max_depth, min_size, depth+1)\n",
    "\n",
    "\n",
    "def build_tree(X, Y, max_depth, min_size):\n",
    "    root = tree_best_split(X, Y)\n",
    "    tree_recursive_split(X, Y, root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the split criteria or the predicted class at each node, we can visualise the decising making process.\n",
    "Both the tree and a a prediction can be implemented recursively, by going from the root to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left_child'], depth+1)\n",
    "        print_tree(node['right_child'], depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "        \n",
    "def tree_predict_single(x, node):\n",
    "    if isinstance(node, dict):\n",
    "        if x[node['index']] < node['value']:\n",
    "            return tree_predict_single(x, node['left_child'])\n",
    "        else:\n",
    "            return tree_predict_single(x, node['right_child'])\n",
    "        \n",
    "    return node\n",
    "\n",
    "def tree_predict_multi(X, node):\n",
    "    Y = np.array([tree_predict_single(row, node) for row in X])\n",
    "    return Y[:, None]  # size: (n_object,) -> (n_object, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our decision tree model on some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)\n",
    "\n",
    "tree = build_tree(X_train, Y_train, 4, 1)\n",
    "Y_pred = tree_predict_multi(X_test, tree)\n",
    "test_accuracy = (Y_pred == Y_test).mean()\n",
    "print('Test Acc: {:.1f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the decision tree in [pre-order](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(lambda x: tree_predict_multi(x, tree), X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experiments\n",
    "The [Cleveland Heart Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset aims at predicting the presence of heart disease based on other available medical information of the patient.\n",
    "\n",
    "Although the whole database contains 76 attributes, we focus on the following 14:\n",
    "1. Age: age in years \n",
    "2. Sex: \n",
    "    * 0 = female\n",
    "    * 1 = male \n",
    "3. Chest pain type: \n",
    "    * 1 = typical angina\n",
    "    * 2 = atypical angina\n",
    "    * 3 = non-anginal pain\n",
    "    * 4 = asymptomatic\n",
    "4. Trestbps: resting blood pressure in mm Hg on admission to the hospital \n",
    "5. Chol: serum cholestoral in mg/dl \n",
    "6. Fasting blood sugar: > 120 mg/dl\n",
    "    * 0 = false\n",
    "    * 1 = true\n",
    "7. Resting electrocardiographic results: \n",
    "    * 0 = normal\n",
    "    * 1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n",
    "    * 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    "8. Thalach: maximum heart rate achieved \n",
    "9. Exercise induced angina:\n",
    "    * 0 = no\n",
    "    * 1 = yes\n",
    "10. Oldpeak: ST depression induced by exercise relative to rest \n",
    "11. Slope: the slope of the peak exercise ST segment\n",
    "    * 1 = upsloping\n",
    "    * 2 = flat \n",
    "    * 3 = downsloping \n",
    "12. Ca: number of major vessels (0-3) colored by flourosopy \n",
    "13. Thal: \n",
    "    * 3 = normal\n",
    "    * 6 = fixed defect\n",
    "    * 7 = reversable defect \n",
    "14. Target: diagnosis of heart disease (angiographic disease status)\n",
    "    * 0 = < 50% diameter narrowing \n",
    "    * 1 = > 50% diameter narrowing\n",
    "    \n",
    "The 14. attribute is the target variable that we would like to predict based on the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared some helper functions to download and pre-process the data in `heart_disease_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heart_disease_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = heart_disease_data.download_and_preprocess()\n",
    "X_train, Y_train, X_test, Y_test = split(X, Y, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0:2])\n",
    "print(Y_train[0:2])\n",
    "\n",
    "# TODO feel free to explore more examples and see if you can predict the presence of a heart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Decision Tree for Heart Disease Prediction \n",
    "Let's build a decision tree model on the training data and see how well it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you are free to make use of code that we provide in previous cells\n",
    "# TODO: play around with different hyper parameters and see how these impact your performance\n",
    "\n",
    "tree = build_tree(X_train, Y_train, 5, 4)\n",
    "Y_pred = tree_predict_multi(X_test, tree)\n",
    "test_accuracy = (Y_pred == Y_test).mean()\n",
    "print('Test Acc: {:.1f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did changing the hyper parameters affect the test performance? Usually hyper parameters are tuned using a hold-out [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Validation_dataset) instead of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression for Heart Disease Prediction\n",
    "\n",
    "Instead of manually going through the data to find possible correlations, let's try training a logistic regression model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you are free to make use of code that we provide in previous cells\n",
    "# TODO: play around with different hyper parameters and see how these impact your performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did your model perform? Was it actually better then guessing? Let's look at the empirical mean of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the problem? Let's have a look at the learned parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.W, model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained sufficiently many steps you'll probably see how some weights are much larger than others. Have a look at what range the parameters were initialized and how much change we allow per step (learning rate). Compare this to the scale of the input features. Here an important concept arises, when we want to train on real world data: \n",
    "[Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling).\n",
    "\n",
    "Let's try applying it on our data and see how it affects our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rescale the input features and train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we did not need any rescaling for the decision tree. Can you think of why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
